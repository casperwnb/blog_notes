* kubernetes
** 架构图
[[file:~/Learn_space/blog_notes/cloud/images/kubuernetes_frame.png][kubernetes架构图]]

kubernetes项目的架构由Master和Node两种节点组成, 分别对应着控制节点和计算节点.

控制节点(Master节点)由3个紧密协作的独立组件组合而成, 分别是负责API服务的kube-apiserver、负责调度
的kube-scheduler、负责容器编排的kube-controller-manager. 整个集群的持久化数据则由kube-apiserver
处理后保存在etcd中.

计算节点上最核心的部分是: kubelet组件. kubelet主要负责同容器运行时打交道. 该交互所依赖的是一个
称作CRI(container Runtime Interface)的远程调用接口, 这个接口定义了容器运行时的各项核心操作, 如
启动一个容器需要的所有参数.
基于此只要容器运行时能够运行标准的容器镜像, 就可以通过实现CRI接入到kubernetes项目中.
而具体的容器运行时比如docker项目, 则一般通过OCI容器运行时规范同底层的Linux操作系统进行交互.

OCI(Open Container Initiative), 定义了容器运行时标准. runC是Docker按照开放容器格式标准(OCF,
Open Container Format)制定的一种具体实现.

kubelet还通过gRPC协议同一个叫做Device Plugin的插件进行交互. 该插件是kubernetes项目用来管理GPU等
宿主机物理设备的主要组件, 也是基于kubernetes项目进行机器学习训练、高性能作业支持等工作必须关注
的能力.

kubelet还可以调用网络插件和存储插件为容器配置网络和持久化存储. 这个两个插件与kubelet进行交互的
接口分别是CNI(container networking interface)和CSI(container storage interface).

运行在大规模集群中的各种任务之间, 实际上存在这各种各样的关系, 这些关系的处理才是作业编排和管理
系统最困难的地方.

kubernetes项目最主要的设计思想是从更宏观的角度, 以统一的方式来定义任务之间的各种关系, 并且为将来
支持更多种类的关系留有余地.

围绕容器和Pod不断向真实的技术场景扩展, 可以摸索出[[file:~/Learn_space/blog_notes/cloud/images/kubernetes_fullscreen.png][kubernetes项目核心功能的"全景图"]].

kubernetes项目中, 推崇的做法是:
1. 通过一个"编排对象", 如Pod, Job, CronJob等来描述视图管理的应用
2. 再为它定义一些"服务对象", 比如: service, secret, horizontal pod autoscaler(自动水平扩展器)等
这种使用方法就是所谓的"声明式API", 这种API对应的"编排对象"和"服务对象"都是kubernetes项目中的API
对象(API Object). 这就是kubernetes最核心的设计理念.

** Pod
Pod 是kubernetes中最基础的一个对象, pod里的容器共享同一个network namespace、同一组数据卷, 从而
达到高效率交换信息的目的.

** service
service服务声明的IP地址等信息是"终生不变"的, 这个Service服务的主要作用就是作为Pod的代理入口(
portal), 从而代替Pod对外暴露一个固定的网络地址.

** secret
这是保存在etcd里的键值对数据, 将credential(数据库的用户名和密码)信息以secret的方式存在etcd里,
kubernetes就会在指定的pod启动时, 自动把secret里的数据以volume的方式挂载到容器里, 此时web应用
就可以访问数据库了.

** Job
Job用来描述一次性运行的Pod(如: 大数据任务).
DaemonSet用来描述每个宿主机上必须且只能运行一个副本的守护进程服务器.
CronJob用于描述定时任务.

* kubeadm
** 介绍
kubeadm是一个容器部署工具. [[https://github.com/kubernetes/kubeadm][github地址]]
kubeadm将kubelet直接运行在宿主机上, 然后使用容器部署其他的kubernetes组件.

kubeadm生成的文件在/etc/kubernetes/pki目录中

** kubeadm init原理
1. kubeadm首先是一系列的检查工作, 以确定这台机器可以用来部署kubernetes, 这一步检查称为"Preflight
   Checkes". 其检查包括了很多方面, 如:
   a. Linux内核的版本必须是否是3.10+
   b. Linux Cgroups模块是否可用
   c. 机器的hosname是否标准,kubernetes项目中,机器的名字以及一切存储在etcd中的api对象, 都必须使用
   标准的DNS命名(RFC 1123)
   d. 用户安装的kubeadm和kubelet的版本是否匹配
   e. 机器上是否已经安装了kubernetes的二进制文件
   f. kubernetes的工作端口10250/10251/10252端口是否已经被占用
   g. ip, mount等Linux命令是否存在
   h. docker是否已经安装
   i. ...
2. 通过Preflight Checkes之后, kubeadm是生成kubernetes对外提供服务所需的各种证书和对应的目录.
   kubernetes对外提供服务器时,除非专门开启"不安全模式", 否则都要通过HTTPS才能访问kube-apiserver.
   kubeadm为kubernetes生成的证书文件都放在master节点的/etc/kubernetes/pki目录下.
   kubecl获取容器日志等streaming时, 需要通过kube-apiserver向kubelet发起请求,
   这个连接也必须是安全的, kubeadm为这一步生成的是apiserver-kubelet-client.crt文件,
   对应的私钥是:apiserver-kubelet-client.key, 可以选择不让kubeadm生成这些证书,
   而是拷贝现有的证书到/etc/kubernetes/pki目录下
3. 证书生成之后,kubedam会为其他组件生成范文kube-apiserver所需的配置文件,
   路径是:/etc/kubernetes/xx.conf
4. 然后kubeadm会为master组件生成pod配置文件.
   在kubernetes中, 有一种特殊的容器启动方法叫做"Static Pod", 允许把要部署的Pod的YAML文件
   放在一个指定的目录里, 这样当kubelet启动时, 会自动检查这个目录, 加载所有的Pod YAML文件,
   然后在这台机器上启动它

   kubelet在kubernetes项目中的地位非常高, 在设计上它就是一个独立的组件.

   在kubeadm中, Master组件的YAML文件会被生成在/etc/kubernetes/manifests路径下.
   一旦这些YAML文件出现在被kubelet监视的/etc/kubernetes/manifests路径下, kubelet就会自动创建
   这些YAML文件中定义的POD, 即master组件的容器.
5. kubeadm就会为集群生成一个bootstrap token
   只要持有这个token, 任何一个安装了kubelet和kubadm的节点, 都可以通过kubeadm join加入集群.
6. token生成后, kubeadm会将ca.crt等master节点的重要信息, 通过configmap的方式保存在etcd当中,
   供后续部署Node节点使用, 这个configmap的名字就是cluster-info
7. kubeadm init的最后一步, 就是安装默认插件. kubernetes默认kube-proxy和DNS这个插件是必须安装的.
   分别用来提供整个集群的服务发现和DNS功能.

** kubeadm join的工作流
kubeadmin join为什么需要这样一个token呢?
因为任何一台机器想要成为kubernetes集群中的一个节点, 就必须在集群的kube-apiserver上注册, 要想
跟apiserver打交道, 这台机器就必须要获取到相应的证书文件. kubeadm至少需要发起一次"不安全模式"的
访问到kube-apiserver, 从而拿到保存在configmap中的cluster-info(保存了apiserver的授权信息), 而
bootstrap token扮演的就是这个过程中的安全验证的角色.

一旦有了cluster-info里的kube-apiserver的地址、端口、证书, kubelet就可以以"安全模式"连接到
apiserver上, 这样一个新的节点就部署完成了.

** 配置kubeadm的部署参数
部署master节点时, 可以使用下面这条指令:
kubeadm init --config kubeadm.yaml  # 通过提供一个YAML文件来创建
kubeadm就会使用上面这些信息替换/etc/kubernetes/manifests/kube-apiserver.yaml里的command字段
里的参数了.

* kubernetes环境搭建
** ubuntu 16.04
1. 安装docker
   apt update && apt install -y docker.io
   使用docker.io源的原因是, docker公司每次发布的最新docker ce产品往往还没有经过kubernetes项目的
   验证, 可能会有兼容性方面的问题.
2. 安装kubelet、kubeadm、kubectl
   在所有节点上安装, [[https://kubernetes.io/docs/setup/independent/install-kubeadm/][官方安装说明]]
   #+BEGIN_SRC text
apt update && apt install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | \
apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

apt update
apt install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl  # 不让其更新, 正式环境需要如此设置
   #+END_SRC

   源可以换成: deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main
3. 在master节点上建立一个kubeadm.yaml文件, 内容如下:
   #+BEGIN_SRC yaml
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
controllerManagerExtraArgs:
  horizontal-pod-autoscaler-use-rest-clients: "true"
  horizontal-pod-autoscaler-sync-period: "10s"
  node-monitor-grace-period: "10s"
apiServerExraArgs:
  runtime-config: "api/all=true"
kubernetesVersion: "stable-1.11"
   #+END_SRC
4. 部署Master节点
   kubeadm init --config kubeadm.yaml

   如果报错: running with swap on is not supported. Please disable swap
   解决方法:
   swapoff -a
   sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

   如果报错: your configuration file uses an old API spec ...
   解决方法:
   将apiVersion的值改为: kubeadm.k8s.io/v1alpha3
   kind的值改为: ClusterConfiguration
   kubernetesVersion的值该为: "stable-1.12"

   kubeadm init执行完之后, 会输出命令:
   kubeadm join <ip>:<port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>
5. 在master节点上执行命令
   #+BEGIN_SRC bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
   #+END_SRC
   kubectl默认会使用$HOME/.kube目录下的授权信息访问kubernetes集群, 如果不这么做, 每次都
   需要通过export KUBECONFIG环境变量告诉kubectl这个安全配置文件的位置.

   Node节点上可以将Master节点上的$HOME/.kube/config拷贝过来即可使用
   echo "source <(kubectl completion bash)" >> ~/.bashrc  # 命令行命令补全提示
6. 部署网络插件
   kubectl apply -f https://git.io/weave-kube-1.6
   # 也可以安装flannel网络插件
   kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yaml
7. 在Node节点执行命令
   执行master节点上的输出 kubeadm join

** centos
1. 设置kubeadm的yum安装源
   #+BEGIN_SRC text
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
# baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
# 安装源换成aliyun
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
# repo_gpgcheck=1
# gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
# exclude=kube*
EOF
setenforce 0

yum install -y epel-release && yum clean all && yum makecache
yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
systemctl enable kubelet && systemctl start kubelet   
   #+END_SRC
** k8s.grc.io不可用的解决方法
docker.io厂库对google的容器做了镜像, 可以通过以下命令拉取相关镜像.
1. docker pull mirrorgooglecontainers/xxxxx
2. docker tag mirrorgooglecontainers/xxxxx k8s.grc.io/xxxxx

** 使用
刚刚部署master节点后, 使用kubectl get nodes查看到该节点的状态是"NotReady".
通过kubectl describe node <node-name> 发现是因为尚未部署任何网络插件.

kubernetes支持容器网络插件, 使用的是一个叫做CNI的通用接口, 它是当前容器网络的事实标准, 市面上
的所有容器网络开源项目都可以通过cni接入kubernetes.

默认情况下, kubernetes的master节点是不能运行用户pod的, 实现这一点是倚靠kubernetes的
Taint/Toleration机制.

其原理是: 一旦某个节点被加上了一个Taint, 即被打上了"污点", 那么所有的Pod就都不能在这个节点上运行,
除非有个别的Pod声明自己能容忍这个污点, 即声明了Toleration, 它才可以在这个节点上运行.

kubectl taint nodes node1 foo=bar:NoSchedule  # 在node1节点上增加一个键值对格式的taint, 其值里的
NoSchedule意味着这个Taint只会在调度新Pod时产生作用, 而不会影响已经在node1上运行的Pod, 哪怕他们
没有toleration.

** Pod声明Toleration
只需在Pod的.yaml文件中的spec部分加入tolerations字段即可:
#+BEGIN_SRC conf
spec:
  tolerations:
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
#+END_SRC
表示该Pod能"容忍"所有键值对为foo=bar的Taint.

注意: 如果某个节点上已经有某个Taint, 但是声明的yaml文件中没有相应的toleration的声明, 则就
一定不会调度到这个节点上, 即Pod具有"洁癖", 默认情况下只喜欢在没有任何taint的节点上运行.

如果想建立单节点的kubernetes, 可以删除默认的taint.
kubectl taint nodes --all node-role.kubernetes.io/master-

kubectl taint nodes <node1> key=value:NoSchedule  # 在node1上添加taint
kubectl taint nodes <node1> key:NoSchedule-  # 删除<node1>上的key的taint

** 部署Dashboard可视化插件
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
部署完成后, 可以查看Dashboard对应的Pod状态:
kubectl get pods -n kube-system
Dashboard是一个web server, 从1.7+后默认只能通过proxy的方式在本地访问, [[https://github.com/kubernetes/dashboard][参考]]

然后运行命令: kubectl proxy, 之后根据提示就可访问了

** 部署容器存储插件
如果在某一台机器上启动一个容器, 显然是无法看到其他机器上的容器在它们的数据卷里写入的文件, 这是
容器最典型的特征之一, 无状态.

容器的持久化存储就是用来保存容器存储状态的重要手段.

存储插件会在容器里挂载一个基于网络或其他机制的远程数据卷, 使得在容器里创建的文件, 实际上是保存在
远程存储服务器上, 或者以分布式的方式保存在多个节点上, 而与当前宿主机没有任何绑定关系. 这样, 无论
在哪个宿主机上启动新的容器, 都可以请求挂载指定的持久化存储卷, 从而访问到数据卷里保存的内容, 这
就是"持久化"的含义.

Rook项目是一个基于Ceph的kubernetes存储插件(后期也在加入对更多存储实现的支持). 不同于Ceph的简单
封装, Rook在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能, 使得该项目成了
一个完整的、生产级可用的容器存储插件.

部署命令:
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml

部署完成后, Rook项目会将自己的Pod放置在由它自己管理的两个Namespace中.
kubectl get pods -n rook-ceph-system
kubectl get pods -n rook-ceph

此时一个基于Rook持久化存储集群就以容器的方式运行起来了, 而在接下来的kubernetes项目上创建的所有
Pod就能够通过Persistent Volumen(PV)和Persistent Volume Claim(PVC)的方式, 在容器里挂载由Ceph提供
的数据卷了.

** kubectl describe的使用
如果对pod有疑问, 可以使用kubectl describe pod <pod-name> 来查看信息, 需要重点注意Events.
在kubernetes执行的过程中, 对api对象的所有重要操作, 都会被记录在这个对象的Events里, 并且显示在
kubectl describe指令返回的结果中. 如果有异常发生, 你一定要第一时间查看这些Events.

* 第一个容器化应用
** 编写kubernetes的配置文件
配置文件可以是Yaml或json格式.

写好配置文件之后, 使用命令: kubectl create -f <config_name>, 例如:
#+BEGIN_SRC yaml nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
#+END_SRC
这样一个yaml文件, 对应到kubernetes中就是一个API Object(API 对象). 当为这个对象的各个字段填好值
并提交给kubernetes后, kubernetes会负责创建出这些对象所定义的容器或其他类型的API资源.

上述yaml文件中Kind字段指定了这个API对象的类型是一个Deployment.
Deployment是一个定义多副本应用(及多个副本pod)的对象, Deployment还负责在Pod定义发生变化时, 对每个
副本进行滚动更新(Rolling update).
定义的Pod副本个数是2(spec.replicas).
Pod模板(spec.template): 定义了该模板要创建的pod细节, 上述文件中, Pod里只定义了一个容器, 该容器的
镜像(spec.containers.image)是nginx:1.7.9, 容器监听端口(containerPort)为80

注意: 像这样使用一种API对象(Deployment)管理另一种API对象(Pod)的方法, 在kubernetes中, 叫作"控制器"
模式.

每个API对象都有一个叫做metadata的字段, 该字段就是API对象的"标识", 即元数据, 是从kubernetes里找到
该对象的主要依据, 其中最主要使用到的字段就是lables.

labels就是一组key-value格式的标签, 像deployment这样的控制器对象就可以通过这个labels字段从
kubernetes中过滤出它所关心的被控制对象. 这个过滤的定义在spec.selector.matchLabels, 一般称之为
Label Selector.

在Metadata中, 还有一个与Labels格式、层级完全相同的字段叫做Annotations, 专门用来携带key-value格式
的内部信息, 所谓内部信息指的是对这些信息感兴趣的是kubernetes组件本身, 而不是用户, 因此大多数
annotations都是在kubernetes运行过程中被自动加在这个api对象上.

一个kubernetes的API对象的定义, 大多可以分为metadata和spec两个部分. metadata存放的是这个对象的元
数据, 对所有api对象来说, 这部分的字段和格式基本上是一样的; spec存放的是属于这个对象独有的定义,
用来描述它所要表达的功能.

** 对Nginx服务进行升级
只需要修改yaml文件, 修改如下:
#+BEGIN_SRC yaml
...
spec:
  containers:
  - name: nginx
    image: nginx:1.8  # 这里从1.7.9升级到1.8
...
#+END_SRC
然后使用kubectl replace -f <yaml_path>指令来完成这个更新.
建议使用kubectl apply 命令来统一进行kubernetes对象的创建和更新操作, 具体做法如下:
kubectl apply -f <yaml_path>

** 在kubernetes中使用Volume
#+BEGIN_SRC yaml nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: "/root/"  # 容器中的挂载目录
          name: nginx-vol
      volumes:
      - name: nginx-vol
        emptyDir: {}
#+END_SRC
以上代码在Pod模板部分添加了一个volumes字段, 定义了这个Pod声明的所有volume, 名字叫做nginx-vol,
类型是emptyDir.

emptyDir: 等同于docker的隐式volume参数, 即不显式声明宿主机目录的volume. 所以kubernetes也会在
宿主机上创建一个临时目录, 这个目录将来就会被绑定挂载到容器所声明的volume目录上.

注意: kubernetes的emptyDir类型, 只是把kubernetes创建的临时目录作为volume的宿主机目录交给了docker.
这么做的原因是kubernetes不想依赖docker自己创建的那个_data目录.
Pod中的容器, 使用的是volumeMount字段来声明自己要挂载哪个volume, 并通过mountPath字段来定义容器内
的volume目录, 比如:"/root"

kubernetes也提供了显式的volume定义, 它叫做hostPath, 例如:
#+BEGIN_SRC yaml
...
volumes:
- name: nginx-vol
  hostPath:
    path: "/var/data"  # 将宿主机上的/var/data挂载到容器中的某个目录下
#+END_SRC
这样容器volume挂载的宿主机目录就变成了/var/data

kubectl delete -f <yaml_path>
