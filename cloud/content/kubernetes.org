* kubernetes
** 架构图
[[file:~/Learn_space/blog_notes/cloud/images/kubuernetes_frame.png][kubernetes架构图]]

kubernetes项目的架构由Master和Node两种节点组成, 分别对应着控制节点和计算节点.

控制节点(Master节点)由3个紧密协作的独立组件组合而成, 分别是负责API服务的kube-apiserver、负责调度
的kube-scheduler、负责容器编排的kube-controller-manager. 整个集群的持久化数据则由kube-apiserver
处理后保存在etcd中.

计算节点上最核心的部分是: kubelet组件. kubelet主要负责同容器运行时打交道. 该交互所依赖的是一个
称作CRI(container Runtime Interface)的远程调用接口, 这个接口定义了容器运行时的各项核心操作, 如
启动一个容器需要的所有参数.
基于此只要容器运行时能够运行标准的容器镜像, 就可以通过实现CRI接入到kubernetes项目中.
而具体的容器运行时比如docker项目, 则一般通过OCI容器运行时规范同底层的Linux操作系统进行交互.

kubelet还通过gRPC协议同一个叫做Device Plugin的插件进行交互. 该插件是kubernetes项目用来管理GPU等
宿主机物理设备的主要组件, 也是基于kubernetes项目进行机器学习训练、高性能作业支持等工作必须关注
的能力.

kubelet还可以调用网络插件和存储插件为容器配置网络和持久化存储. 这个两个插件与kubelet进行交互的
接口分别是CNI(container networking interface)和CSI(container storage interface).

运行在大规模集群中的各种任务之间, 实际上存在这各种各样的关系, 这些关系的处理才是作业编排和管理
系统最困难的地方.

kubernetes项目最主要的设计思想是从更宏观的角度, 以统一的方式来定义任务之间的各种关系, 并且为将来
支持更多种类的关系留有余地.

围绕容器和Pod不断向真实的技术场景扩展, 可以摸索出[[file:~/Learn_space/blog_notes/cloud/images/kubernetes_fullscreen.png][kubernetes项目核心功能的"全景图"]].

kubernetes项目中, 推崇的做法是:
1. 通过一个"编排对象", 如Pod, Job, CronJob等来描述视图管理的应用
2. 再为它定义一些"服务对象", 比如: service, secret, horizontal pod autoscaler(自动水平扩展器)等
这种使用方法就是所谓的"声明式API", 这种API对应的"编排对象"和"服务对象"都是kubernetes项目中的API
对象(API Object). 这就是kubernetes最核心的设计理念.

** Pod
Pod 是kubernetes中最基础的一个对象, pod里的容器共享同一个network namespace、同一组数据卷, 从而
达到高效率交换信息的目的.

** service
service服务声明的IP地址等信息是"终生不变"的, 这个Service服务的主要作用就是作为Pod的代理入口(
portal), 从而代替Pod对外暴露一个固定的网络地址.

** secret
这是保存在etcd里的键值对数据, 将credential(数据库的用户名和密码)信息以secret的方式存在etcd里,
kubernetes就会在指定的pod启动时, 自动把secret里的数据以volume的方式挂载到容器里, 此时web应用
就可以访问数据库了.

** Job
Job用来描述一次性运行的Pod(如: 大数据任务).
DaemonSet用来描述每个宿主机上必须且只能运行一个副本的守护进程服务器.
CronJob用于描述定时任务.

* kubeadm
** 介绍
kubeadm是一个容器部署工具. [[https://github.com/kubernetes/kubeadm][github地址]]

** 安装
kubeadm把kubelet直接运行在宿主机上, 然后使用容器部署其他的kubernetes组件.

1. apt update && apt install -y apt-transport-https
2. curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
3. cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
   deb http://apt.kubernetes.io/ kubernetes-xenial main
   EOF
4. apt update
5. apt install -y docker-ce kubeadm

** kubeadm init原理
1. kubeadm首先是一系列的检查工作, 以确定这台机器可以用来部署kubernetes, 这一步检查称为"Preflight
   Checkes". 其检查包括了很多方面, 如:
   a. Linux内核的版本必须是否是3.10+
   b. Linux Cgroups模块是否可用
   c. 机器的hosname是否标准. kubernetes项目中, 机器的名字以及一切存储在etcd中的api对象, 都必须使用
   标准的DNS命名(RFC 1123)
   d. 用户安装的kubeadm和kubelet的版本是否匹配
   e. 机器上是否已经安装了kubernetes的二进制文件
   f. kubernetes的工作端口10250/10251/10252端口是否已经被占用
   g. ip, mount等Linux命令是否存在
   h. docker是否已经安装
   i. ...
2. 通过Preflight Checkes之后, kubeadm是生成kubernetes对外提供服务所需的各种证书和对应的目录.
   kubernetes对外提供服务器时, 除非专门开启"不安全模式", 否则都要通过HTTPS才能访问kube-apiserver.
   kubeadm为kubernetes生成的证书文件都放在master节点的/etc/kubernetes/pki目录下.
   kubecl获取容器日志等streaming时, 需要通过kube-apiserver向kubelet发起请求,
   这个连接也必须是安全的, kubeadm为这一步生成的是apiserver-kubelet-client.crt文件,
   对应的私钥是:apiserver-kubelet-client.key, 可以选择不让kubeadm生成这些证书,
   而是拷贝现有的证书到/etc/kubernetes/pki目录下
3. 证书生成之后,kubedam会为其他组件生成范文kube-apiserver所需的配置文件,
   路径是:/etc/kubernetes/xx.conf

4. 然后kubeadm会为master组件生成pod配置文件.
   在kubernetes中, 有一种特殊的容器启动方法叫做"Static Pod", 允许把要部署的Pod的YAML文件
   放在一个指定的目录里, 这样当kubelet启动时, 会自动检查这个目录, 加载所有的Pod YAML文件,
   然后在这台机器上启动它

   kubelet在kubernetes项目中的地位非常高, 在设计上它就是一个独立的组件.

   在kubeadm中, Master组件的YAML文件会被生成在/etc/kubernetes/manifests路径下.
   一旦这些YAML文件出现在被kubelet监视的/etc/kubernetes/manifests路径下, kubelet就会自动创建
   这些YAML文件中定义的POD, 即master组件的容器.
5. kubeadm就会为集群生成一个bootstrap token
   只要持有这个token, 任何一个安装了kubelet和kubadm的节点, 都可以通过kubeadm join加入集群.
6. token生成后, kubeadm会将ca.crt等master节点的重要信息, 通过configmap的方式保存在etcd当中,
   供后续部署Node节点使用, 这个configmap的名字就是cluster-info
7. kubeadm init的最后一步, 就是安装默认插件. kubernetes默认kube-proxy和DNS这个插件是必须安装的.
   分别用来提供整个集群的服务发现和DNS功能.

** kubeadm join的工作流
kubeadmin join为什么需要这样一个token呢?
因为任何一台机器想要成为kubernetes集群中的一个节点, 就必须在集群的kube-apiserver上注册, 要想
跟apiserver打交道, 这台机器就必须要获取到相应的证书文件. kubeadm至少需要发起一次"不安全模式"的
访问到kube-apiserver, 从而拿到保存在configmap中的cluster-info(保存了apiserver的授权信息), 而
bootstrap token扮演的就是这个过程中的安全验证的角色.

一旦有了cluster-info里的kube-apiserver的地址、端口、证书, kubelet就可以以"安全模式"连接到
apiserver上, 这样一个新的节点就部署完成了.

** 使用
kubeadm init  # 创建一个master节点
kubeadm join <master节点的ip和端口>  # 将一个Node加入到当前集群中.

** 配置kubeadm的部署参数
部署master节点时, 可以使用下面这条指令:
kubeadm init --config kubeadm.yaml  # 通过提供一个YAML文件来创建

kubeadm就会使用上面这些信息替换/etc/kubernetes/manifests/kube-apiserver.yaml里的cmmand字段
里的参数了.
* kubernetes环境搭建
** ubuntu 16.04
1. 安装docker
   apt update && apt install -y docker.io
   使用docker.io源的原因是, docker公司每次发布的最新docker ce产品往往还没有经过kubernetes项目的
   验证, 可能会有兼容性方面的问题.
2. 安装kubelet、kubeadm、kubectl
   在所有节点上安装
   #+BEGIN_SRC text
apt update && apt install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | \
apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

apt update
apt install -y kubelet kubeadm kubectl
   #+END_SRC

   源可以换成: deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main
3. 在master节点上建立一个kubeadm.yaml文件, 内容如下:
   #+BEGIN_SRC yaml
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
controllerManagerExtraArgs:
  horizontal-pod-autoscaler-use-rest-clients: "true"
  horizontal-pod-autoscaler-sync-period: "10s"
  node-monitor-grace-period: "10s"
apiServerExraArgs:
  runtime-config: "api/all=true"
kubernetesVersion: "stable-1.11"
   #+END_SRC
4. 部署Master节点
   kubeadm init --config kubeadm.yaml

   如果报错: running with swap on is not supported. Please disable swap
   解决方法:
   swapoff -a
   sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

   kubeadm init执行完之后, 会输出命令:
   kubeadm join <ip>:<port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>
5. 在master节点上执行命令
   #+BEGIN_SRC bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
   #+END_SRC
   kubectl默认会使用$HOME/.kube目录下的授权信息访问kubernetes集群, 如果不这么做, 每次都
   需要通过export KUBECONFIG环境变量告诉kubectl这个安全配置文件的位置.

   Node节点上可以将Master节点上的$HOME/.kube/config拷贝过来即可使用
   echo "source <(kubectl completion bash)" >> ~/.bashrc  # 命令行命令补全提示
6. 部署网络插件
   kubectl apply -f https://git.io/weave-kube-1.6
   # 也可以安装flannel网络插件
   kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yaml
7. 在Node节点执行命令
   执行master节点上的输出 kubeadm join

** 使用
刚刚部署master节点后, 使用kubectl get nodes查看到该节点的状态是"NotReady".
通过kubectl describe node <node-name> 发现是因为尚未部署任何网络插件.

kubernetes支持容器网络插件, 使用的是一个叫做CNI的通用接口, 它是当前容器网络的事实标准, 市面上
的所有容器网络开源项目都可以通过cni接入kubernetes.

默认情况下, kubernetes的master节点是不能运行用户pod的, 实现这一点是倚靠kubernetes的
Taint/Toleration机制.

其原理是: 一旦某个节点被加上了一个Taint, 即被打上了"污点", 那么所有的Pod就都不能在这个节点上运行,
除非有个别的Pod声明自己能容忍这个污点, 即声明了Toleration, 它才可以在这个节点上运行.

kubectl taint nodes node1 foo=bar:NoSchedule  # 在node1节点上增加一个键值对格式的taint, 其值里的
NoSchedule意味着这个Taint只会在调度新Pod时产生作用, 而不会影响已经在node1上运行的Pod, 哪怕他们
没有toleration.

** Pod声明Toleration
只需在Pod的.yaml文件中的spec部分加入tolerations字段即可:
#+BEGIN_SRC conf
spec:
  tolerations:
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
#+END_SRC
表示该Pod能"容忍"所有键值对为foo=bar的Taint.

注意: 如果某个节点上已经有某个Taint, 但是声明的yaml文件中没有相应的toleration的声明, 则就
一定不会调度到这个节点上, 即Pod具有"洁癖", 默认情况下只喜欢在没有任何taint的节点上运行.

如果想建立单节点的kubernetes, 可以删除默认的taint.
kubectl taint nodes --all node-role.kubernetes.io/master-

kubectl taint nodes <node1> key=value:NoSchedule  # 在node1上添加taint
kubectl taint nodes <node1> key:NoSchedule-  # 删除<node1>上的key的taint

** 部署Dashboard可视化插件
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
部署完成后, 可以查看Dashboard对应的Pod状态:
kubectl get pods -n kube-system
Dashboard是一个web server, 从1.7+后默认只能通过proxy的方式在本地访问, [[https://github.com/kubernetes/dashboard][参考]]

** 部署容器存储插件
如果在某一台机器上启动一个容器, 显然是无法看到其他机器上的容器在它们的数据卷里写入的文件, 这是
容器最典型的特征之一, 无状态.

容器的持久化存储就是用来保存容器存储状态的重要手段.

存储插件会在容器里挂载一个基于网络或其他机制的远程数据卷, 使得在容器里创建的文件, 实际上是保存在
远程存储服务器上, 或者以分布式的方式保存在多个节点上, 而与当前宿主机没有任何绑定关系. 这样, 无论
在哪个宿主机上启动新的容器, 都可以请求挂载指定的持久化存储卷, 从而访问到数据卷里保存的内容, 这
就是"持久化"的含义.

Rook项目是一个基于Ceph的kubernetes存储插件(后期也在加入对更多存储实现的支持). 不同于Ceph的简单
封装, Rook在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能, 使得该项目成了
一个完整的、生产级可用的容器存储插件.

部署命令:
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml

部署完成后, Rook项目会将自己的Pod放置在由它自己管理的两个Namespace中.
kubectl get pods -n rook-ceph-system
kubectl get pods -n rook-ceph

此时一个基于Rook持久化存储集群就以容器的方式运行起来了, 而在接下来的kubernetes项目上创建的所有
Pod就能够通过Persistent Volumen(PV)和Persistent Volume Claim(PVC)的方式, 在容器里挂载由Ceph提供
的数据卷了.

** 命令
kubectl get namespaces  # 获取命名空间

kubectl get pods # 获取默认命名空间下的pods信息
kubectl get pod --all-namespaces -o wide  # 查看所有pod的状态, -o wide 可以展示更多信息

kubectl get nodes  # 查看节点信息
kubectl describe node master  # 查看master节点的详细信息
kubectl get pods -n kube-system  # 查看节点上各个系统Pod的状态, kube-system是kubernetes项目预留
的系统Pod的工作空间(不是Linux Namespace, 只是kubernetes划分不同工作空间的单位)

kubectl taint nodes --all node-role.kubernetes.io/master-  # 删除默认的taint, 其中的"-"表示移除
所有以"node-role.kubernetes.io/master"为键的taint

kubectl describe pods  # 查看默认命名空间下的pods的信息
kubectl describe pod <podname> --namespace=<namespace>  # 查看pod的具体信息
kubectl get pods -l app=nginx  # -l的值是labels标签中定义的键-元素对

kubectl logs <POD_NAME>  # 查看某个pod的日志信息
kubectl exec -ti <podname> <cmd>  # 在某个pod中执行命令

kubectl get services  # 查看服务状态
kubectl label pod <podname> app=v1  # 给podname添加对应的标签
kubeadm token list  # 查看kubeadmin token信息

** kubectl describe的使用
如果对pod有疑问, 可以使用kubectl describe pod <pod-name> 来查看信息, 需要重点注意Events.
在kubernetes执行的过程中, 对api对象的所有重要操作, 都会被记录在这个对象的Events里, 并且显示在
kubectl describe指令返回的结果中. 如果有异常发生, 你一定要第一时间查看这些Events.

* 第一个容器化应用
** 编写kubernetes的配置文件
配置文件可以是Yaml或json格式.

写好配置文件之后, 使用命令: kubectl create -f <config_name>, 例如:
#+BEGIN_SRC yaml nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
#+END_SRC
这样一个yaml文件, 对应到kubernetes中就是一个API Object(API 对象). 当为这个对象的各个字段填好值
并提交给kubernetes后, kubernetes会负责创建出这些对象所定义的容器或其他类型的API资源.

上述yaml文件中Kind字段指定了这个API对象的类型是一个Deployment.
Deployment是一个定义多副本应用(及多个副本pod)的对象, Deployment还负责在Pod定义发生变化时, 对每个
副本进行滚动更新(Rolling update).
定义的Pod副本个数是2(spec.replicas).
Pod模板(spec.template): 定义了该模板要创建的pod细节, 上述文件中, Pod里只定义了一个容器, 该容器的
镜像(spec.containers.image)是nginx:1.7.9, 容器监听端口(containerPort)为80

注意: 像这样使用一种API对象(Deployment)管理另一种API对象(Pod)的方法, 在kubernetes中, 叫作"控制器"
模式.

每个API对象都有一个叫做metadata的字段, 该字段就是API对象的"标识", 即元数据, 是从kubernetes里找到
该对象的主要依据, 其中最主要使用到的字段就是lables.

labels就是一组key-value格式的标签, 像deployment这样的控制器对象就可以通过这个labels字段从
kubernetes中过滤出它所关心的被控制对象. 这个过滤的定义在spec.selector.matchLabels, 一般称之为
Label Selector.

在Metadata中, 还有一个与Labels格式、层级完全相同的字段叫做Annotations, 专门用来携带key-value格式
的内部信息, 所谓内部信息指的是对这些信息感兴趣的是kubernetes组件本身, 而不是用户, 因此大多数
annotations都是在kubernetes运行过程中被自动加在这个api对象上.

一个kubernetes的API对象的定义, 大多可以分为metadata和spec两个部分. metadata存放的是这个对象的元
数据, 对所有api对象来说, 这部分的字段和格式基本上是一样的; spec存放的是属于这个对象独有的定义,
用来描述它所要表达的功能.

** 对Nginx服务进行升级
只需要修改yaml文件, 修改如下:
#+BEGIN_SRC yaml
...
spec:
  containers:
  - name: nginx
    image: nginx:1.8  # 这里从1.7.9升级到1.8
...
#+END_SRC
然后使用kubectl replace -f <yaml_path>指令来完成这个更新.
建议使用kubectl apply 命令来统一进行kubernetes对象的创建和更新操作, 具体做法如下:
kubectl apply -f <yaml_path>

** 在kubernetes中使用Volume
#+BEGIN_SRC yaml nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: "/root/"
          name: nginx-vol
      volumes:
      - name: nginx-vol
        emptyDir: {}
#+END_SRC
以上代码在Pod模板部分添加了一个volumes字段, 定义了这个Pod声明的所有volume, 名字叫做nginx-vol,
类型是emptyDir.

emptyDir: 等同于docker的隐式volume参数, 即不显式声明宿主机目录的volume. 所以kubernetes也会在
宿主机上创建一个临时目录, 这个目录将来就会被绑定挂载到容器所声明的volume目录上.

注意: kubernetes的emptyDir类型, 只是把kubernetes创建的临时目录作为volume的宿主机目录交给了docker.
这么做的原因是kubernetes不想依赖docker自己创建的那个_data目录.
Pod中的容器, 使用的是volumeMount字段来声明自己要挂载哪个volume, 并通过mountPath字段来定义容器内
的volume目录, 比如:"/root"

kubernetes也提供了显式的volume定义, 它叫做hostPath, 例如:
#+BEGIN_SRC yaml
...
volumes:
- name: nginx-vol
  hostPath:
    path: "/var/data"
#+END_SRC
这样容器volume挂载的宿主机目录就变成了/var/data

kubectl delete -f <yaml_path>

* Pod
** Pod简介
docker的本质就是Namespace做隔离, Cgroups做限制, rootfs做文件系统. kubernetes提出pod的概念.
容器就是未来云计算系统中的进程, 容器镜像就是这个系统里的".exe"安装包, 因此kubernetes就类似于
"操作系统".

pstree -g  # 展示当前系统中正在运行的进程的树状结构.

在一个操作系统里, 进程并不是"孤苦伶仃"地独自运行的, 而是以进程组的方式有"原则的"组织在一起, 在
这个进程的树状图中, 每个进程后面括号里的数字就是它的进程组ID(process group id, PGID).

对于操作系统来说, 这样的进程组更方便管理, Linux操作系统只需要将信号发送给一个进程组, 那么该进程组
中的所有进程都会收到这个信号而终止运行.

kubernetes项目做的, 就是将"进程组"的概念映射到了容器技术中, 并使其成为了这个云计算"操作系统"里的
"一等公民".
kubernetes项目的调度器统一按照Pod而非容器的资源需求进行计算的.

容器间的紧密协作, 可以称为"超亲密关系". 这些具有"超亲密关系"容器的典型特征包括但不限于: 互相之间
会发生直接的文件交换、使用localhost或socket文件进行本地通信、会发生非常频繁的远程调用、需要共享
某些linux namespace(如一个容器要加入另一个容器的network namespace)等等.

** Pod实现原理
Pod在kubernetes项目里还有更重要的意义, 就是: 容器设计模式
Pod只是一个逻辑概念, 即kubernetes正真处理的还是宿主机操作系统上Linux容器的Namespace和Cgroups, 并
不存在一个所谓的Pod的边界或者隔离环境. Pod的本质是一组共享了某些资源的容器. 具体说: Pod里的所有
容器共享的是同一个Network namespace, 并且可以声明共享同一个Volume.

理解Pod的本质: 实际上是在扮演传统基础设施里"虚拟机"的角色, 容器这是这个"虚拟机"中运行的用户程序

虽然可以通过docker run的--net, --volumes-form也能实现, 但这样容器B就必须比容器A先启动, 这样一个
Pod里的多个容器就不是对等关系, 而是拓扑关系了.

在kubernetes项目里, pod的实现需要使用一个中间容器, 这个容器叫作Infra容器, 在Pod中, Infra容器
永远都是第一个被创建的容器, 而其他用户定义的容器, 则通过Join Network namespace的方式, 与infra容器
关联在一起, 参见: [[file:~/Learn_space/blog_notes/cloud/images/pod.png][Pod示意图]]

Infra容器一定要占用极小的资源, 所以它使用的是一个非常特殊的镜像, 叫做: k8s.gcr.io/pause, 该镜像
是用一个汇编语言编写, 永远处于"暂停"状态的容器, 解压后的大小也只有100~200kb左右.
Infra容器"Hold住"Network namespace后, 用户容器就可以加入到infra容器的network namespace当中了.
如果查看这些容器在宿主机上的namespace文件, 它们指向的值一定是完全一样的.

对于Pod里的容器A和B来说:
1. 它们可以直接使用localhost进行通信
2. 它们看到的网络设备跟Infra容器看到的完全一样
3. 一个Pod只有一个IP地址, 即该Pod的network namespace对应的ip地址
4. 其他的所有网络资源都是一个pod一份, 并且被该pod中的所有容器共享
5. Pod是生命周期只跟Infra容器一致, 而容器A和B无关

对于同一个Pod里的所有容器, 它们的进出流量也可以认为都是通过Infra容器完成的.
因此如果将来要为kubernetes开发一个网络插件时, 应该重点考虑的是如何配置这个Pod的network namespace,
而不是每个用户容器如何使用你的网络配置, 这是没有意义的.

Pod这种"超亲密关系"容器的设计思想, 实际上就是希望当用户想在一个容器里跑多个功能并不相关的应用
时, 应该优先考虑它们是不是更应该被描述成一个Pod里的多个容器.

** 例子
1. 最典型的例子是: WAR包与Web服务器
   #+BEGIN_SRC yaml
apiVersion: v1
kind: Pod
metadata:
  name: javaweb-2
spec:
  initContainers:
  - image: geektime/sample:v2
    name: war
    command: ["cp", "/sample.war", "/app"]
    volumeMounts:
    - mountPath: /app
      name: app-volume
  containers:
  - image: geektime/tomcat:7.0
    name: tomcat
    command: ["sh", "-c", "/root/apache-tomcat/bin/start.sh"]
    volumeMounts:
    - mountPath: "/root/apache-tomcat/webapps"
      name: app-volume
    ports:
    - containerPort: 8080
      hostPort: 8001
  volumes:
  - name: app-volume
    emptyDir: {}
   #+END_SRC
   注意: War包容器的类型是一个Init Container类型的容器, 在Pod中, 所有InitContainer都会比
   spec.containers定义的用户容器先启动, 并且InitContainer容器会按顺序逐一启动, 直到它们都启动并且
   退出了, 用户容器才会启动.

   像这样用一种"组合"操作, 正是容器设计模式里最常用的一种模式, 它叫做sidecar.
   sidecar指的就是可以在一个Pod中, 启动一个辅助容器, 来完成一些独立于主进程(主容器)之外的工作.
2. 容器的日志收集
   需要不断地把日志文件输出到容器的/var/log目录中, 这时就可以把一个pod里的volume挂载到应用容器
   的/var/log目录上. 这样, sidecar容器就只需要做一件事儿, 就是不断地从自己的/var/log目录里读取
   日志文件, 转发到MongoDB货Elasticsearch中存储起来.
3. Istio项目使用sidecar容器完成微服务治理的原理.

* 在线交互环境
https://kubernetes.io/docs/tutorials/kubernetes-basics

kubeadm join 10.211.55.4:6443 --token xctjyz.9b2dw4f3deyw4cdc --discovery-token-ca-cert-hash sha256:1db8f64748875d822a7b06a961b01f3427628295008102e4f8599f61af9ea8ff
