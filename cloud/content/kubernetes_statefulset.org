* StatefulSet - 拓扑状态
** 介绍
deployment实际上并不足以覆盖所有的应用编排问题. 原因是deployment认为一个应用的所有Pod是完全一样的,
他们之间没有顺序, 也无所谓运行在哪台宿主机上, 需要的时候deployment就可以通过Pod模板创建新的Pod,
不需要的时候就删掉任意一个Pod. 但实际场景中, 并不是所有的应用都可以满足这样的要求.
尤其是分布式应用, 多个实例之间往往有依赖关系, 如: 主从关系、主备关系等.

还有就是数据存储类应用, 它的多个实例, 往往都会在本地磁盘上保存一份数据, 而这些示例一旦被杀掉, 即便
重新建立出来, 示例与数据之间的对应关系也丢失了, 从而导致应用失败.

这种实例之间有不对等关系以及实例对外部数据有依赖关系的应用, 就被称为"有状态应用
(stateful application)".

得益于"控制器模式"的设计思想, kubernetes在deployment的基础上扩展除了对"有状态应用"的初步支持,
这个编排功能就是"StatefulSet".

它把真实世界里的应用状态, 抽象为了两种情况:
1. 拓扑状态
   此情况下应用的多个实例之间不是完全对等的关系, 这些应用实例必须按照某些顺序启动, 比如应用的主
   节点A要先于节点B启动, 而如果把A和B两个Pod删除掉, 它们再次被创建出来时也必须严格按照这个顺序才行
   并且新创建出来的Pod必须和原来Pod的网络标识一样, 这样原先的访问者才能使用同样的方法访问到这个
   新Pod
2. 存储状态
   应用的多个实例分别绑定了不同的存储数据, 对于这些应用实例来说, Pod A第一次读取到的数据和隔了
   一段时间后再次读取到的数据应该是同一份, 哪怕期间Pod A被重新创建过.

StatefulSet的核心功能, 就是通过某种方式记录这些状态, 然后在Pod被重新创建时,
能够为新Pod恢复这些状态.

** Headless Service
Service是kubernetes项目中用来将一组Pod暴露给外界访问的一种机制. 该Service又是如何被访问的呢?
第一种方式: 以Service的VIP(virtual IP, 即: 虚IP)方式. 比如: 访问10.0.23.1这个Service的IP地址时
10.0.23.1就是一个VIP, 会将请求转发到该Service所代理的某一个Pod上.

第二种方式: 以Service的DNS方式. 如: 访问"my-svc.mynamespace.svc.cluster.local"这条DNS记录, 就可以
访问到名叫my-svc的service所代理的某个Pod.

Service DNS的方式具体还可以分两种处理方法:
1. Normal Service
   访问my-svc.mynamespace.svc.cluster.local解析到的, 正是my-svc这个service的VIP, 后面就跟VIP方式
   一致了
2. Headless Service
   访问my-svc.mynamespace.svc.cluster.local解析到的, 直接就是my-svc代理的某一个Pod的IP地址.
   此处Headless Service不需要分配一个VIP, 而是直接以DNS记录的方式解析出被代理Pod的IP地址.

   #+BEGIN_SRC yaml  Headless Service的Yaml文件
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx

spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
   #+END_SRC
Headless Service仍然是一个标准Service的Yaml文件, 只是其clusterIP字段的值为None, 即该service没有
一个VIP作为"头". 这也是headless的含义, 所以该service被创建后并不会被分配一个VIP, 而是以DNS记录的
方式暴露出它所代理的Pod. 而它所代理的Pod, 依然采用Label Selector机制选择出来, 即所有携带了
app=nginx标签的Pod, 都会被这个service代理起来.

当按照此方法创建了一个Headless service之后, 它所代理的所有Pod的Ip地址都会被绑定一个这样格式的
DNS记录, 如下:
<pod-name>.<svc-name>.<namespace>.svc.cluster.local
这个DNS记录正是kubernetes项目为Pod分配的唯一的"可解析身份(resolvable identity)".
有了这个"可解析身份", 只要知道了一个Pod的名字, 以及它对应的Service的名字, 就可以非常确定地通过
这条DNS记录放到Pod的IP地址.

** StatefulSet又是如何使用这个DNS记录来维持Pod的拓扑状态
#+BEGIN_SRC yaml StatefulSet的Yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
#+END_SRC
serviceName字段指明了StatefulSet控制器, 在执行控制循环的时候, 使用nginx这个Headless Service来保证
Pod的"可解析身份".

创建之后, 可以使用 kubectl get statefulset web  来查看信息.
kubectl get pods -w -l app=nginx  # -w即watch功能, 实时查看statefulset创建两个有状态实例的过程
statefulSet给它所管理的所有Pod的名字进行了编号, 编号规则是使用"-"进行连接, 编号从0开始累加, 与
statefulset的每个Pod实例一一对应, 绝不重复.
更重要的是, 这些Pod的创建, 也是严格按照标号顺序进行的, 如: 在web-0进入到Running状态并且细分
状态称为Ready前, web-1一直处于pending状态.
当Pod都进入Running状态后, 就可以查看他们各自唯一的"网络身份了". 如:
kubectcl exec web-0 -- sh -c 'hostname'

试着以DNS的方式, 访问一下这个Headless Service:
kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh  # 启动一个一次性的
Pod, 然后在这个Pod的容器里尝试用nslookup命令解析一下Pod对应的Headless Service
nslookup web-0.nginx

注意: 在执行nslookup时, 可能会无法解析该域名, 但是是可以ping通的, 多试几次就会有几次会有正确结果
或者可以使用ubuntu镜像来做测试, 效果会好一些.

此时如果删除这"有状态应用"的Pod, kubectl delete pod -l app=nginx.
kubernetes会按照原先编号的顺序, 创建出两个新的Pod, 并且kubernetes依然为他们分配了与原来相同的
"网络身份".

通过这种严格的对应规则, StatefulSet就保证了Pod网络标识的稳定性.

通过这种方法, kubernetes就成功的将Pod的拓扑状态(如: 哪个节点先启动, 哪个节点后启动), 按照Pod的
"名字+编号"的方式固定了下来. kubernetes还为每个Pod提供了一个固定并且唯一的访问入口, 即这个Pod
对应的DNS记录.

这些状态在Statefulset的整个生命周期里都会保持不变, 绝不会因为对应Pod的删除或者重新创建而失效.

对于"有状态应用"实例的访问, 必须使用DNS记录或者hostname的方式, 绝对不应该直接访问这些pod的IP地址

* StatefulSet - 存储状态
** 存储状态的管理机制
主要使用的是: Persistent Volume Claim
#+BEGIN_SRC yaml ceph RBD类型Volume的Pod
apiVersion: v1
kind: Pod
metadata:
  name: rdb
spec:
  containers:
    - image: kubernetes/pause
      name: rbd-rw
      volumentMounts:
      - name: rbdpd
        mountPath: /mnt/rbd
  volumes:
  - name: rbdpd
    rbd:
      monitors:
      - '10.16.154.78:6789'
      - '10.16.154.82:6789'
      pool: kube
      image: foo
      fsType: ext4
      readOnly: true
      user: admin
      keyring: /etc/ceph/keyring
      imageformat: "2"
      imagefeatures: "layering"
#+END_SRC
该配置文件存在两个问题:
1. 如果不懂得Ceph RBD的使用方法, 那么这个Pod里Volumes字段就不容易看懂
2. 这个ceph rbd对应的存储服务器的地址、用户名、授权文件的位置, 也被轻易的暴露了

这也就是kubernetes引入了一组叫做Persistent Volume Claim(PVC)和Persistent Volume(PV)的API对象,
大大的降低了用户声明和使用持久化Volume的门槛.

** PVC - 使用volume的一个例子
1. 定义一个PVC, 声明想要的Volume的属性
   #+BEGIN_SRC yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
   #+END_SRC
   该PVC对象里, 不需要任何关于Volume细节的字段, 只有描述性的属性和定义.
   accessModes: ReadWriteOnce表示该Volume的挂载方式是可读写, 并且只能被挂载在一个节点上
   而非被多个节点共享.
   [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes][Volume支持的accessMode]]
2. 在应用的Pod中, 声明使用这个PVC
   #+BEGIN_SRC yaml
apiVersion: v1
kind: Pod
metadata:
  name: pv-pod
spec:
  containers:
  - name: pv-container
    image: nginx
    ports:
    - containerPort: 80
      name: "http-server"
    volumeMounts:
    - mountPath: "/usr/share/nginx/html"
      name: pv-storage
  volumes:
  - name: pv-storage
    persistentVolumeClaim:
      claimName: pv-claim
   #+END_SRC
   该Pod的volumes定义中, 只需要声明它的类型是persistentVolumeClaim, 然后指定PVC的名字, 完全不必
   关心volume本身的定义.

   此时, 只要创建这个pvc对象, kubernetes就会自动为它绑定一个符合条件的volume. 这些符合条件的
   volume是来自于运维人员维护的PV(Persistent Volume)对象. PV的一个Yaml示例:
   #+BEGIN_SRC yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume
  labels:
    type: local
spec:
  capacity:
    storage: 10Gi
  rbd:
    monitors:
    - '10.16.154.78:6789'
    - '10.16.154.82:6789'
    pool: kube
    image: foo
    fsType: ext4
    readOnly: true
    user: admin
    keyring: /etc/ceph/keyring
    imageformat: "2"
    imagefeatures: "layering"
   #+END_SRC
   该PV对象的spec.rbd字段, 正是前面介绍过的ceph rbd volume的详细定义. 这样kubernetes就会为我们
   刚刚创建的PVC对象绑定这个PV.

   kubernetes中PVC和PV的设计, 类似于"接口"和"实现"的思想.
   开发者只要知道并会使用"接口", 即PVC. 运维人员则负责给"接口"绑定具体的实现, 即: PV.

   PVC, PV的设计也使得statefulset对存储状态的管理成为了可能.
   #+BEGIN_SRC yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
   #+END_SRC
   为StatefulSet额外添加了一个volumeClaimTemplates字段, 凡是被这个StatefulSet管理的Pod, 都会声明
   一个对应的PVC, 该PVC就是来自volumeClaimTemplates这个模板字段. 并且该PVC的名字会分配一个与这
   个Pod完全一致的编号
   
   该自动创建的PVC与PV绑定成功后, 就会进入Bound状态, 意味着该Pod可以挂载并使用这个PV了
   
   PVC就是一种特殊的Volume, 只是PVC具体是什么类型的Volume, 是要在跟某个PV绑定之后才知道.
   PVC与PV的绑定得以实现的前提是: 运维人员已经在系统里创建好了符合条件的PV, 或者kubernetes集群
   运行在公有云上, 此时kubernetes就会通过Dynamic Provisioning的方式自动创建与PVC匹配的PV.
   kubectl get pvc -l app=nginx # 获取pvc
   
   PVC都以"<PVC名字>-<StatefulSet名字>-<编号>"的方式命名, 并且处于Bound状态.

** StatefulSet控制器恢复Pod的过程
1. 将一个Pod, 如web-0删除之后, 该Pod对应的PVC和PV并不会被删除, 且这个Volume里已经写入的数据
   依然会保存在远程存储服务里.
2. StatefulSet控制器发现一个名叫"web-0"的Pod消失了, 所以控制器就会重新创建一个新、名字还是叫做
   web-0的Pod来"纠正"这个不一致的情况.
   注意: 该新Pod对象的定义里, 声明使用的PVC的名字还是叫做www-web-0. 该PVC的定义还是来自于PVC模板
   这是statefulset创建Pod的标准流程

   在新web-0 Pod被创建出来之后, kubernetes为它查找名叫www-web-0的PVC时, 就会直接找到旧Pod遗留下来
   的同名PVC, 进而找到跟这个PVC绑定在一起的PV, 这样新Pod就可以挂载到旧Pod对应的那个Volume, 且获取
   到保存在volume里的数据.

   通过该方式, kubernetes的statefulset就实现了对应用存储状态的管理.

总结一下Statefulset的工作原理:
1. statefulset的控制器直接管理的是Pod
2. 通过headless service为这些有编号的Pod, 在DNS服务器中生成带有同样编号的DNS记录
3. StatefulSet还为每个Pod分配并创建一个同样编号的PVC.
* StatefulSet - 搭建Mysql主从
** 搭建要求
1. 是一个"主从复制(Master-Slave Replication)"的Mysql集群
2. 有一个主节点, 多个从节点
3. 从节点需要能水平扩展
4. 所有的写操作只能在主节点上执行
5. 读操作可以在所有节点上执行

在常规环境里, 部署一个主从模式的Mysql集群的主要难点在于: 如何让从节点能够拥有主节点的数据, 即:
如何配置主从节点的复制与同步.

** 使用物理机搭建的过程
1. 在安装好mysql的master节点后, 第一步工作就是通过xtrabackup将master节点的数据备份到指定目录.
   xtrabackup是业界主要使用的开源mysql备份和恢复工具.
   这一步会自动在目标目录里生成一个备份信息文件, 名叫xtrabackup_binlog_info, 该文件一般会包含
   如下两个信息:
   TheMaster-bin.000001 481
   这两个信息会在接下来配置slave节点的时候会用到
2. 配置slave节点
   slave节点在第一次启动前, 需要先把master节点的备份数据连同备份信息文件, 一起拷贝到自己的数据
   目录下(/var/lib/mysql), 然后执行如下sql语句:
   #+BEGIN_SRC mysql
CHANGE MASTER TO
MASTER_HOST="$masterip",
MASTER_USER="xxx",
MASTER_PASSWORD="XXX",
MASTER_LOG_FILE="TheMaster-bin.000001",
MASTER_LOG_POS=481
   #+END_SRC
   MASTER_LOG_FILE和MASTER_LOG_POS就是该备份对应的二进制日志文件的名称和开始的位置(偏移量),
   也正是xtrabackup_binlog_info文件里的那两部分内容
3. 启动slave节点
   执行 START SLAVE;
4. 在这个集群中添加更多的slave节点
   注意: 新添加的slave节点的备份数据, 来自于已经存在的slave节点
   此时需要将slave节点的数据备份在指定目录, 该备份操作会自动生成另一种备份信息文件,
   名叫: xtrabackuup_slave_info, 内容是MASTER_LOG_FILE和MASTER_LOG_POS两个字段的值
   然后执行前面一样的"CHANGE MASTER TO"和"START SLAVE"命令, 来初始化并启动这个新Slave节点
** 使用容器搭建的问题分析
从上可以看出, 将部署mysql集群的流程迁移到kubernetes项目上, 需要能够"容器化"地解决下面的"三大问题"
1. Master节点和slave节点需要有不同的配置文件
2. Master节点和slave节点需要能够传输备份信息文件
3. 在slave节点第一次启动之前, 需要执行一些初始化sql操作

由于mysql本身同时拥有拓扑状态(主从节点)和存储状态(mysql保存在本地的数据), 需要通过statefulset来
解决这3个问题.

*** 问题1
只需要给主从节点分别准备两份不同的mysql配置文件, 然后根据Pod的序号(index)挂载进去即可.
这样的配置文件信息应该保存在ConfigMap里供Pod使用, 其定义如下:
#+BEGIN_SRC yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  labels:
    app: mysql
data:  # data部分是key-value格式的, master.cnf就是key, "|"后面的部分就是value
# 这份数据挂载进master节点对应的Pod后, 就会在volume目录里生成一个master.cnf的文件
  master.cnf: |
    # 主节点mysql的配置文件
    [mysqld]
    log-bin  # 使用二进制日志文件的方式进行主从复制, 标准设置
  slave.cnf: |
    # 从节点mysql的配置信息
    [mysqld]
    super-read-only  # 从节点拒绝除了主节点的数据同步操作之外的所有写操作, 即: 对用户是只读的
#+END_SRC

接下来需要创建两个Service来提供statefulset.
#+BEGIN_SRC yaml
# master.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
    - name: mysql
      port: 3306
  clusterIP: None
  selector:
    app: mysql

# slave.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
spec:
  ports:
    - name: mysql
      port: 3306
  selector:
    app: mysql
#+END_SRC
这两个service都代理了所有携带app=mysql标签的Pod, 端口映射都是用service的3306端口对应Pod的3306端口
master的service是一个headless service, 其作用是通过Pod分配DNS记录来固定它的拓扑状态, 如:
mysql-0.mysql和mysql-1.mysql这样的dns名字, 其中编号为0的节点就是该例子中的主节点.

slave的service是一个常规的service.
所有用户的读请求, 都必须访问第二个service被自动分配的DNS记录, 即"mysql-read"(也可以访问这个
service的VIP), 这样请求就可以被转发到任意一个mysql的主节点或从节点上.
所有用户的写请求, 则必须直接以DNS记录的方式访问到mysql的主节点, 即"mysql-0.mysql"这条DNS记录.

kubernetes中的所有service、Pod对象, 都会被自动分配同名的DNS记录.

*** 问题2
推荐的做法是: 先搭建框架, 再完善细节. 其中Pod部分如何定义是完善细节时的重点.
首先为statefulset对象规划一个大致的框架, 如下:
#+BEGIN_SRC yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: xxx
      - name: xxx
      containers:
      - name: mysql(xxx)
      - name: xtrabackup(xxx)
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
#+END_SRC
定义了selector表示该statefulset要管理的pod必须携带app=mysql标签, 声明要使用的headless service
是mysql.
replicas为3, 表示定义的mysql集群有3个节点, 一个master节点和两个slave节点.

statefulset管理的有状态应用的多个实例, 也都是通过同一份pod模板创建出来的, 使用的是同一个docker
镜像, 即如果应用要求不同节点的镜像不一样, 就不能使用Statefulset了, 需要考虑使用Operator.

该集群还需要管理存储状态, 需要通过volumeClaimTemplate(PVC模板)来为每个Pod定义PVC.
ReadWriteOnce指定了该存储的属性为可读写, 并且一个PV只允许挂载在一个宿主机上,
将来这个pv对应的volume就会充当mysql pod的存储数据目录.

现在重点设计一下这个statefulset的Pod模板, 即template字段.
由于statefulset管理的Pod都来自于同一个镜像, 这就要求在编写Pod时, 一定要保持清醒, 进行如下的分析:
1. 如果这个Pod是master节点, 要怎么做
2. 如果这个Pod是slave节点, 要怎么做

1. 从configmap中, 获取mysql的pod对应的配置文件
   需要进行一个初始化操作, 根据节点的角色是master还是slave, 为pod分配对应的配置文件, 此外,
   mysql还要去集群中的每个节点都有一个唯一的ID文件, 叫做server-id.cnf.
   
   根据之前的知识, 这些初始化操作适合通过InitContainer来完成, 先定义InitContainer,
   #+BEGIN_SRC yaml
...
# template.spec
initContainers:
- name: init-mysql
  image: mysql:5.7
  command:
  - bash
  - "-c"
  - |
    set -ex
    # 从Pod的序号, 生成server-id
    [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
    ordinal=${BASH_REMATCH[1]}
    echo [mysqld] > /mnt/conf.d/server-id.cnf
    # 由于server-id=0有特殊含义, 给ID加100来避开它
    echo server-id=$((100+$ordinal)) >> /mnt/conf.d/server-id.cnf
    # 如果Pod序号是0, 说明是Master节点, 从configmap里把master的配置文件拷贝到/mnt/conf.d/目录
    # 否则拷贝slave的配置文件
    if [[ $ordinal -eq 0 ]]; then
      cp /mnt/config-map/master.cnf /mnt/conf.d/
    else
      cp /mnt/config-map/slave.cnf /mnt/conf.d/
    fi
  volumeMounts:
  - name: conf
    mountPath: /mnt/conf.d
  - name: config-map
    mountPath: /mnt/config-map
   #+END_SRC
   在这个init-mysql的InitContainer的配置中, 它从Pod的hostname里读取到了Pod序号, 以此作为mysql
   节点的server-id. 然后init-mysql通过序号判断当前pod到底是master还是slave, 从而拷贝相应的配置
   文件. 当InitContainer复制完配置文件退出后, 后面启动的mysql容器只需要直接声明挂载这个名叫
   conf的volume, 它所需要的.cnf配置文件已经出现在里面了.
2. 在slave Pod启动前, 从master或者slave pod里拷贝数据库到自己的目录下
   需要再定义第二个InitContainer
   #+BEGIN_SRC yaml
...
# template.spec.initContainers
- name: clone-mysql
  image: gcr.io/google-samples/xtrabackup:1.0
  command:
  - bash
  - "-c"
  - |
    set -ex
    # 拷贝操作只需要在第一次启动时进行, 所以如果数据已经存在, 跳过
    [[ -d /var/lib/mysql/mysql ]] && exit 0
    # master节点(序号为0) 不需要做这个操作
    [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
    ordinal=${BASH_REMATCH[1]}
    [[ $ordinal -eq 0 ]] && exit 0
    # 使用ncat指令, 远程地从前一个节点拷贝数据到本地
    ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
    # 执行--prepare, 拷贝来的数据就可以用作恢复了
    xtrabackup --prepare --target-dir=/var/lib/mysql
  volumeMounts:
  - name: data
    mountPath: /var/lib/mysql
    subPath: mysql
  - name: conf
    mountPath: /etc/mysql/conf.d
   #+END_SRC
   在clone-mysql的initcontainer的配置中, 使用xtrabackup镜像, 运行思路是:
   首先做了一个判断, 即当初始化所需的数据(/var/lib/mysql/mysql目录)已经存在, 或者当前Pod是master
   节点的时候, 就不需要做拷贝.
   然后使用linux自带的ncat指令, 向DNS记录为"mysql-<当前序号减1>.mysql"的Pod, 即当前pod的前一个
   pod发起数据传输请求, 并且直接使用xbstream指令将收到的备份数据保存在/var/lib/mysql目录下.
   3307是一个特殊端口, 运行着一个专门负责备份mysql数据的辅助进程.
   数据的传输过程可以使用其他来传输, 如: scp, rsync等
   容器里的/var/lib/mysql目录实际上是一个名为data的PVC.
   数据拷贝之后, clone-mysql容器还需要对/var/lib/mysql目录执行一句xtrabackup --prepare操作, 目的
   是让拷贝进来的数据进入一致性状态, 这样这些数据才能被用作数据恢复

   接下来就可以定义mysql容器启动mysql服务了.
   由于statefulset里的素有Pod都来自用同一个pod模板, 所以还需要考虑该mysql容器的启动命令在master
   和slave两种情况下的不同.

   有了docker镜像, 在pod里声明一个master角色的mysql容器直接执行mysql启动命令即可.
   但如果这个pod是一个第一次启动的slave节点, 在执行mysql启动命令前, 就需要使用前面InitContainer
   拷贝来的备份数据进行初始化.
   但容器是一个单进程模型, 谁负责在slave容器启动之前执行初始化sql语句呢. 这就是之前提到的问题3
*** 问题3
可以为这个mysql容器额外定义一个sidecar容器, 来完成这个操作, 定义如下:
#+BEGIN_SRC yaml
...
# template.spec.containers
- name: xtrabackup
  image: gcr.io/google-samples/xtrabackup:1.0
  ports:
  - name: xtrabackup
    containerPort: 3307
  command:
  - bash
  - "-c"
  - |
    set -ex
    cd /var/lib/mysql
    
    # 从备份信息文件里读取MASTER_LOG_FILE和MASTER_LOG_POS这两个字段的值, 用来拼装集群初始化sql
    if [[ -f xtrabackup_slave_info ]]; then
      # 如果xtrabackup——slave_info 文件存在, 说明这个备份数据来自于另一个slave节点,
      # 这种情况下, xtrabackup工具在备份的时候就已经在这个文件里自动生成了"CHANGE MASTER TO"
      # 语句, 所以只需要把这个文件重命名为change_master_to.sql.in, 后面直接使用即可
      mv xtrabackup_slave_info change_master_to.sql.in
      # 所以, 也用不着xtrabackup_binlog_info了
      rm -f xtrabackup_binlog_info
    elif [[ -f xtrabackup_binlog_info ]]; then
      # 如果只存在xtrabackup_binlog_info文件, 说明备份来自master节点, 需要解析这个备份文件,
      # 读取所需的两个字段的值
      [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
      rm xtrabackup_binlog_info
      # 把两个字段的值拼装成sql, 写入change_master_to.sql.in文件
      echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
            MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
    fi
    
    # 如果存在change_master_to.sql.in, 就意味着需要做集群初始化工作
    if [[ -f change_master_to.sql.in ]]; then
      # 一定要等mysql容器启动之后才能进行下一步连接mysql的操作
      echo "waiting for mysqld to be ready(accepting connections)"
      until mysql -h 127.0.0.1 -e "select 1"; do sleep 1; done
      
      echo "Initializing replcation from clone position"
      # 将文件change_master_to.sql.in改个名字, 防止这个container重启的时候, 因为找到了
      # change_master_to.sql.in而重新执行一遍初始化流程
      mv change_master_to.sql.in change_master_to.sql.orig
      # 使用change_master_to.sql.orig的内容, 组成一个完整的初始化和启动slave的sql语句
      mysql -h 127.0.0.1 <<EOF
      $(<change_master_to.sql.orig),
      MASTER_HOST="mysql-0.mysql",
      MASTER_USER="root",
      MASTER_PASSWORD="",
      MASTER_CONNECT_RETRY=10;
      START SLAVE;
      EOF
    fi
    
    # 使用ncat监听3307端口, 其作用是在收到传输请求的时候, 直接执行"xtrabackup --backup"命令
    # 备份mysql的数据并发送给请求者
    exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
    "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
  volumeMounts:
  - name: data
    mountPath: /var/lib/mysql
    subPath: mysql
  - name: conf
    mountPath: /etc/mysql/conf.d
  #+END_SRC
这个名叫xtrabackup的sidecar容器的启动命令里, 其实实现了两部分工作.
1. mysql节点的初始化工作
   该初始化需要使用的sql是sidecar容器拼装出来、保存在一个名为change_master_to.sql.in中
   此时sidecar容器就可以执行初始化了.
   需要注意的是: Pod里的容器并没有先后顺序, 所以在执行初始化sql之前, 必须先检查mysql服务是否已经
   可用.
   初始化操作完成后, 需要删除掉前面用到的这些备份信息, 否则下次重启时, 又会执行一次数据恢复和集群
   初始化的操作, 这是不正确的.
2. 在完成mysql节点的初始化后, 这个sidecar容器的第二个工作, 则是启动一个数据传输服务.
   sidecar容器使用ncat命令启动一个工作在3307端口上的网络发送服务, 一旦收到数据传输请求时,
   sidecard容器就会调用xtrabackup --backup指令备份当前mysql的数据, 然后把备份数据返回给请求
   者, 这也是为什么在initcontainer里定义数据拷贝的时候, 访问的是"上一个mysql节点"的3307端口.
   
*** 定义Pod里的主角 - mysql容器
#+BEGIN_SRC yaml
...
# template.spec
containers:
- name: mysql
  image: mysql:5.7
  env:
  - name: MYSQL_ALLOW_EMPTY_PASSWORD
    value: "1"
  ports:
  - name: mysql
    containerPort: 3306
  volumeMounts:
  - name: data
    mountPath: /var/lib/mysql
    subPath: mysql
  - name: conf
    mountPath: /etc/mysql/conf.d
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
  livenessProbe:
    exec:
      command: ["mysqladmin", "ping"]
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
  readinessProbe:
    exec:
      # 通过TCP连接的方式进行健康检查
      command: ["mysql", "-h", "127.0.0.1", "-e", "select 1"]
    initialDelaySeconds: 5
    periodSeconds: 2
    timeoutSeconds: 1
#+END_SRC

*** 在kubernetes集群里创建满足条件的PV
#+BEGIN_SRC yaml
apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 3

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  clusterNamespace: rook-ceph
#+END_SRC
此处用到了storageclass来完成这个操作, 其作用是自动的为集群里存在的每一个pvc调用存储插件(rook)创建
对应的PV, 从而省去了手动创建PV的机械劳动.

[[https://github.com/oracle/kubernetes-website/blob/master/docs/tasks/run-application/mysql-statefulset.yaml][完整的yaml文件]]
